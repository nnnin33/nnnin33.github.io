

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="求职学习路线 20260127版本需要的技术：   技能维度 具体要求 备注    编程语言 Python (绝对核心), C++ (高性能计算&#x2F;端侧&#x2F;机器人必备) Python用于模型开发，C++用于部署加速和底层控制。   深度学习框架 PyTorch (主流), TensorFlow&#x2F;JAX (特定场景) 不仅要会调包，要懂底层的自动求导和算子实现。   模型架">
<meta property="og:type" content="article">
<meta property="og:title" content="AI学习路线--学习路线20260127">
<meta property="og:url" content="http://example.com/2026/01/27/AI%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF-%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF20260127/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="求职学习路线 20260127版本需要的技术：   技能维度 具体要求 备注    编程语言 Python (绝对核心), C++ (高性能计算&#x2F;端侧&#x2F;机器人必备) Python用于模型开发，C++用于部署加速和底层控制。   深度学习框架 PyTorch (主流), TensorFlow&#x2F;JAX (特定场景) 不仅要会调包，要懂底层的自动求导和算子实现。   模型架">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2026-01-27T12:33:40.000Z">
<meta property="article:modified_time" content="2026-02-19T08:32:21.638Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="学习路线">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>AI学习路线--学习路线20260127 - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="AI学习路线--学习路线20260127"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2026-01-27 20:33" pubdate>
          January 27, 2026 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5.6k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          47 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">AI学习路线--学习路线20260127</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="求职学习路线-20260127版本"><a href="#求职学习路线-20260127版本" class="headerlink" title="求职学习路线 20260127版本"></a>求职学习路线 20260127版本</h1><h3 id="需要的技术："><a href="#需要的技术：" class="headerlink" title="需要的技术："></a>需要的技术：</h3><table>
<thead>
<tr>
<th><strong>技能维度</strong></th>
<th><strong>具体要求</strong></th>
<th><strong>备注</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>编程语言</strong></td>
<td><strong>Python</strong> (绝对核心), <strong>C++</strong> (高性能计算&#x2F;端侧&#x2F;机器人必备)</td>
<td>Python用于模型开发，C++用于部署加速和底层控制。</td>
</tr>
<tr>
<td><strong>深度学习框架</strong></td>
<td><strong>PyTorch</strong> (主流), TensorFlow&#x2F;JAX (特定场景)</td>
<td>不仅要会调包，要懂底层的自动求导和算子实现。</td>
</tr>
<tr>
<td><strong>模型架构</strong></td>
<td><strong>Transformer</strong> 架构详解 (Attention机制), Diffusion Model (生成式), LoRA&#x2F;QLoRA (微调技术)</td>
<td>面试必问原理。需掌握如何微调(Fine-tuning)开源大模型(Llama, Qwen等)。</td>
</tr>
<tr>
<td><strong>工程落地 (MLOps)</strong></td>
<td><strong>RAG (检索增强生成)</strong>, LangChain&#x2F;LlamaIndex, Docker&#x2F;K8s, 模型推理加速 (TensorRT, vLLM)</td>
<td><strong>这是目前企业招聘最看重的部分</strong>：如何把模型做成稳定、低成本的服务。</td>
</tr>
<tr>
<td><strong>数学基础</strong></td>
<td>线性代数, 概率论, 优化算法 (SGD, Adam)</td>
<td>决定了你能否看懂最新论文并复现。</td>
</tr>
</tbody></table>
<p><strong>如果做具身智能&#x2F;机器人：</strong> 需要掌握 ROS&#x2F;ROS2（机器人操作系统）、仿真环境（Isaac Sim, MuJoCo）以及控制理论基础。</p>
<p><strong>如果做端侧部署：</strong> 需要了解硬件架构（GPU&#x2F;NPU&#x2F;FPGA）、CUDA编程、内存管理。</p>
<p><strong>如果做智能体应用：</strong> 需要理解Prompt Engineering的高级技巧（CoT, ReAct），以及向量数据库（Vector DB）的使用。</p>
<h3 id="现代-AI-系统-5-个维度。"><a href="#现代-AI-系统-5-个维度。" class="headerlink" title="现代 AI 系统 5 个维度。"></a>现代 AI 系统 <strong>5 个维度</strong>。</h3><p>如果求职“AI 工程师”或“AI 架构师”，需要对这五层都有所了解，但通常只需精通其中 1-2 层。</p>
<h4 id="1-认知层-The-Cognitive-Layer-——-“大脑”"><a href="#1-认知层-The-Cognitive-Layer-——-“大脑”" class="headerlink" title="1. 认知层 (The Cognitive Layer) —— “大脑”"></a>1. 认知层 (The Cognitive Layer) —— “大脑”</h4><p>这是系统的核心算力来源，负责理解、推理和生成。</p>
<ul>
<li><strong>基础大模型 (Base LLM):</strong> 系统的底座。<ul>
<li><em>例子：</em> GPT-4, Claude 3.5, Llama 3 (开源), Qwen (阿里千问)。</li>
</ul>
</li>
<li><strong>微调适配器 (Adapters&#x2F;PEFT):</strong> 针对特定任务的技能包（正如我们刚才讨论的）。<ul>
<li><em>例子：</em> 一个专门写 SQL 代码的 LoRA，或者一个专门做医疗诊断的 Adapter。</li>
</ul>
</li>
<li><strong>路由模块 (Model Router):</strong> 一个“分流器”。<ul>
<li><em>作用：</em> 简单问题（打招呼）丢给便宜的小模型（Llama-8B），复杂问题（写架构方案）丢给昂贵的大模型（GPT-4）。这叫“混合专家策略 (MoE)”的工程化应用。</li>
</ul>
</li>
</ul>
<h4 id="2-记忆与数据层-Memory-Data-Layer-——-“图书馆”"><a href="#2-记忆与数据层-Memory-Data-Layer-——-“图书馆”" class="headerlink" title="2. 记忆与数据层 (Memory &amp; Data Layer) —— “图书馆”"></a>2. 记忆与数据层 (Memory &amp; Data Layer) —— “图书馆”</h4><p>AI 模型的记忆只有“当下”，这一层负责给它提供长期记忆和私有知识。</p>
<ul>
<li><strong>向量数据库 (Vector Database):</strong> RAG 的核心。<ul>
<li><em>作用：</em> 把企业的 PDF、文档变成向量存起来，供 AI 随时检索。</li>
<li><em>技术栈：</em> Pinecone, Milvus, ChromaDB。</li>
</ul>
</li>
<li><strong>上下文管理 (Context Management):</strong> 短期记忆。<ul>
<li><em>作用：</em> 记住用户 5 分钟前说了什么。如果对话太长超过了模型限制，需要用算法（如滑动窗口、摘要压缩）来取舍保留哪些记忆。</li>
</ul>
</li>
<li><strong>知识图谱 (Knowledge Graph):</strong> 结构化知识。<ul>
<li><em>作用：</em> 向量数据库擅长模糊搜索，知识图谱擅长逻辑关联（比如：A 是 B 的子公司，B 的产品是 C）。</li>
</ul>
</li>
</ul>
<h4 id="3-行动与编排层-Agentic-Orchestration-Layer-——-“指挥官”"><a href="#3-行动与编排层-Agentic-Orchestration-Layer-——-“指挥官”" class="headerlink" title="3. 行动与编排层 (Agentic &amp; Orchestration Layer) —— “指挥官”"></a>3. 行动与编排层 (Agentic &amp; Orchestration Layer) —— “指挥官”</h4><p>这是目前最火的“智能体”所在的层级，负责把静态的思考转化为动态的行动。</p>
<ul>
<li><strong>编排框架 (Orchestration Framework):</strong> 把模型、Prompt、内存串联起来的胶水代码。<ul>
<li><em>技术栈：</em> <strong>LangChain</strong> (最通用), <strong>LlamaIndex</strong> (专注数据检索), <strong>AutoGen</strong> (多智能体协作)。</li>
</ul>
</li>
<li><strong>工具集 (Tools &#x2F; Skills):</strong> AI 的“手和脚”。<ul>
<li><em>例子：</em> 谷歌搜索工具、Python 代码解释器（用于计算）、SQL 连接器（用于查库）、API 请求工具（用于订票）。</li>
</ul>
</li>
<li><strong>规划器 (Planner):</strong> 任务拆解逻辑。<ul>
<li><em>逻辑：</em> 收到“写一份市场报告”的指令 -&gt; 拆解为 1.搜集数据 2.分析竞品 3.撰写摘要 -&gt; 依次执行。</li>
</ul>
</li>
</ul>
<h4 id="4-基础设施与算力层-Infrastructure-Layer-——-“流水线”"><a href="#4-基础设施与算力层-Infrastructure-Layer-——-“流水线”" class="headerlink" title="4. 基础设施与算力层 (Infrastructure Layer) —— “流水线”"></a>4. 基础设施与算力层 (Infrastructure Layer) —— “流水线”</h4><p>涉及底层性能优化。</p>
<ul>
<li><strong>模型推理服务 (Model Serving):</strong> 如何让模型跑得快？<ul>
<li><em>技术栈：</em> <strong>vLLM</strong> (高吞吐), <strong>TensorRT-LLM</strong> (NVIDIA 加速), <strong>Ollama</strong> (本地运行)。</li>
</ul>
</li>
<li><strong>量化与压缩 (Quantization):</strong> 为了省钱省显存。<ul>
<li><em>技术：</em> 把 FP16（16位浮点数）的模型压缩成 INT4（4位整数），在损失微小精度的情况下让速度翻倍。</li>
</ul>
</li>
</ul>
<h4 id="5-监控与评估层-Eval-Ops-Layer-——-“质检部”"><a href="#5-监控与评估层-Eval-Ops-Layer-——-“质检部”" class="headerlink" title="5. 监控与评估层 (Eval &amp; Ops Layer) —— “质检部”"></a>5. 监控与评估层 (Eval &amp; Ops Layer) —— “质检部”</h4><p>在传统软件里叫 DevOps，在 AI 里叫 <strong>LLMOps</strong>。这是企业最看重的，因为 AI 经常“胡说八道”（幻觉）。</p>
<ul>
<li><strong>评估框架 (Evaluation):</strong> 考试系统。<ul>
<li><em>作用：</em> 每次模型更新后，自动跑一套题（RAGAS, TruLens），打分看回答准不准。</li>
</ul>
</li>
<li><strong>护栏 (Guardrails):</strong> 安全网。<ul>
<li><em>作用：</em> 在 AI 开口说话前，检查有没有泄露隐私、有没有说脏话。如果检测到，直接拦截并替换回复。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="工作流"><a href="#工作流" class="headerlink" title="工作流"></a>工作流</h3><p>当用户问一句：“查一下昨天的机床故障率，并生成图表。”</p>
<ol>
<li><strong>护栏层</strong>：检查问题是否合规。</li>
<li><strong>编排层 (Agent)</strong>：分析意图，决定需要调用“数据库工具”和“画图工具”。</li>
<li><strong>认知层 (Model)</strong>：生成 SQL 查询语句。</li>
<li><strong>行动层 (Tools)</strong>：执行 SQL，拿到数据。</li>
<li><strong>基础设施层 (Serving)</strong>：快速推理，调用 Python 画图库生成图片。</li>
<li><strong>记忆层</strong>：把这次对话存入历史记录。</li>
<li><strong>输出</strong>：用户看到了一张图表。</li>
</ol>
<p><strong>对于求职来说：</strong></p>
<ul>
<li><strong>算法岗</strong> 关注第 1 层（模型架构、微调）。</li>
<li><strong>应用开发岗</strong> 关注第 2、3 层（RAG、LangChain、智能体逻辑）。</li>
<li><strong>AI 架构&#x2F;部署岗</strong> 关注第 4、5 层（推理加速、稳定性）。</li>
</ul>
<h1 id="python用于模型开发"><a href="#python用于模型开发" class="headerlink" title="python用于模型开发"></a>python用于模型开发</h1><h3 id="处理数据"><a href="#处理数据" class="headerlink" title="处理数据"></a>处理数据</h3><p><strong>具体工作：</strong></p>
<ul>
<li>使用 <code>Pandas</code> 处理结构化数据（如Excel表格）。</li>
<li>使用 <code>OpenCV</code> 或 <code>Pillow</code> 读取和预处理图像（裁剪、缩放、归一化）。</li>
<li><strong>最关键的：</strong> 编写 <code>DataLoader</code>（数据加载器）。你需要写Python代码来决定：数据怎么分批次（Batch）？怎么打乱顺序（Shuffle）？怎么在CPU上预处理好然后快速喂给GPU？</li>
</ul>
<p><strong>代码体现：</strong> 继承 <code>Dataset</code> 类，重写 <code>__getitem__</code> 方法。</p>
<p><strong>分批次</strong>：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">train_loader</span> = DataLoader(dataset, batch_size=<span class="hljs-number">32</span>, ...)<br></code></pre></td></tr></table></figure>

<p>显存允许范围内通常 32-256 之间</p>
<p><strong>打乱顺序</strong>：</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs routeros">训练集：通常设为 <span class="hljs-literal">True</span><br>train_loader = DataLoader(dataset, <span class="hljs-attribute">batch_size</span>=32, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">True</span>, <span class="hljs-built_in">..</span>.)<br><br>测试/验证集：通常设为 <span class="hljs-literal">False</span><br><br>test_loader = DataLoader(dataset, <span class="hljs-attribute">batch_size</span>=32, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">False</span>, <span class="hljs-built_in">..</span>.)<br></code></pre></td></tr></table></figure>

<p>如果不打乱，它会根据数据的顺序（而非特征）来作弊</p>
<p>什么时候不打乱？</p>
<ul>
<li>验证&#x2F;测试时： 为了保证每次测试结果的一致性，或者我们需要对应输出结果和原始文件名时，不打乱。</li>
<li>时序数据（如股市预测、语音识别）： 这种情况下数据的前后关系是特征的一部分，不能随意打乱（通常用专门的采样器处理）。</li>
</ul>
<p><strong>在CPU上预处理好然后快速喂给GPU</strong>:</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros">train_loader = DataLoader(<br>    dataset,<br>    <span class="hljs-attribute">batch_size</span>=32,<br>    <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">True</span>,<br>    <span class="hljs-attribute">num_workers</span>=4,    # 核心：多进程加载<br>    <span class="hljs-attribute">pin_memory</span>=<span class="hljs-literal">True</span>   # 核心：锁页内存<br>)<br></code></pre></td></tr></table></figure>



<ul>
<li><p><strong><code>num_workers</code>（多进程并行）：</strong></p>
<ul>
<li><p><em>原理：</em> 默认是 <code>0</code>，意味着主进程自己干活（读取图片 -&gt; 解码 -&gt; 增强 -&gt; 转Tensor）。设为 <code>4</code>，就是主进程只负责训练，另外雇了 4 个“小工”在后台并行处理数据。</p>
</li>
<li><p><em>怎么定？</em> 取决于你的 CPU 核心数。</p>
<ul>
<li>通常设置为：<strong>CPU 核心数</strong> 或 <strong>CPU 核心数 &#x2F; 2</strong>。<ul>
<li><em>注意：</em> 数值不是越大越好，进程开销过大反而变慢。对于你的 Jetson 开发板，CPU 较弱，设置 2-4 可能比较合适。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong><code>pin_memory=True</code>（锁页内存）：</strong></p>
<ul>
<li><p><em>原理：</em> 计算机内存分为“可分页（Pageable）”和“锁页（Pinned）”。CPU 向 GPU 传输数据时，如果数据在锁页内存中，可以直接通过 DMA（直接内存访问）高速传输，不需要 CPU 参与复制。</p>
</li>
<li><p><em>决策：</em> <strong>只要你用 GPU 训练，几乎永远设为 True</strong>。它能显著减少数据从内存拷贝到显存的时间。</p>
</li>
</ul>
</li>
</ul>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-keyword">from</span> torch.utils.data import DataLoader<br><br>假设 dataset 已经定义好了<br><br>dataset = MyCustomDataset(<span class="hljs-built_in">..</span>.)<br><br>train_loader = DataLoader(<br>    dataset,<br>    <span class="hljs-attribute">batch_size</span>=64,      # 决策：根据显存大小调整，显存爆了就调小<br>    <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">True</span>,       # 决策：训练必须打乱，防止过拟合<br>    <span class="hljs-attribute">num_workers</span>=8,      # 决策：利用多核CPU并行预处理数据，防止GPU等待<br>    <span class="hljs-attribute">pin_memory</span>=<span class="hljs-literal">True</span>,    # 决策：加速内存到显存的传输<br>    <span class="hljs-attribute">drop_last</span>=<span class="hljs-literal">True</span>      # 额外技巧：如果数据总量除不尽Batch size，扔掉最后那点零头，保证形状一致<br>)<br><br>实际训练循环中使用<br><br><span class="hljs-keyword">for</span> batch_data, batch_labels <span class="hljs-keyword">in</span> train_loader:<br>    # 这一步将数据真正搬运到 GPU<br>    inputs = batch_data.<span class="hljs-keyword">to</span>(<span class="hljs-string">&#x27;cuda&#x27;</span>) <br>    labels = batch_labels.<span class="hljs-keyword">to</span>(<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>    <br><span class="hljs-comment"># 此时数据已经是按 Batch 切分好的 Tensor 了</span><br><span class="hljs-comment"># ... 模型前向传播 ...</span><br></code></pre></td></tr></table></figure>



<p>在 <code>__getitem__</code> 里有巨大的发挥空间：</p>
<ul>
<li><strong>信号处理：</strong> 你可以在这里加入 FFT（快速傅里叶变换）、小波变换，把时域信号变成频域信号再喂给 AI。</li>
<li><strong>物理先验：</strong> 你可以计算一些物理量（如功率 &#x3D; 电流 * 电压），作为一个新特征喂给模型，而不是让 AI 自己去瞎猜物理定律。</li>
</ul>
<h3 id="构建网络"><a href="#构建网络" class="headerlink" title="构建网络"></a>构建网络</h3><p>把论文里的数学公式或架构图，翻译成Python类（Class）。</p>
<ul>
<li><strong>具体工作：</strong><ul>
<li>利用深度学习框架（如 <strong>PyTorch</strong>）搭建神经网络的“骨架”。</li>
<li>你需要定义每一层是什么（卷积层？全连接层？Attention层？），以及数据在这些层之间是如何流动的（Forward Path）。</li>
<li><strong>求职现状：</strong> 在工业界，很少从零写一个全新的模型。更多的情况是<strong>加载开源模型（如Llama, ResNet）</strong>，然后用Python修改它的最后几层，或者插入一些适配器（Adapter）来适应新任务。</li>
</ul>
</li>
</ul>
<blockquote>
<p>我想知道构建网络时需要定义的每一层是什么都是怎么定义的（卷积层？全连接层？Attention层？），以及数据在这些层之间是如何流动的（Forward Path）？<br>然后还想知道加载开源模型（如Llama, ResNet），然后用Python修改它的最后几层，或者插入一些适配器（Adapter）来适应新任务这种怎么修改？添加什么样子的adapter？</p>
</blockquote>
<h4 id="第一部分：手搓模型——层怎么定义？数据怎么流？"><a href="#第一部分：手搓模型——层怎么定义？数据怎么流？" class="headerlink" title="第一部分：手搓模型——层怎么定义？数据怎么流？"></a>第一部分：手搓模型——层怎么定义？数据怎么流？</h4><p>在 PyTorch 中，所有的神经网络模块都继承自 <code>nn.Module</code> 类。</p>
<h5 id="1-定义零件-init"><a href="#1-定义零件-init" class="headerlink" title="1. 定义零件 (__init__)"></a>1. 定义零件 (<code>__init__</code>)</h5><p>这是你仓库里的库存。你需要在这里声明你会用到哪些“数学运算单元”。</p>
<ul>
<li><strong>卷积层 (<code>nn.Conv2d</code>)</strong>：提取空间特征。<ul>
<li><em>机械类比：</em> 像一个<strong>滤波器</strong>或<strong>扫描探头</strong>，在图片上滑动，检测边缘、纹理。</li>
<li><em>定义：</em> <code>nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)</code>。意思是输入3通道（RGB），输出16种特征图，扫描窗口是3x3。</li>
</ul>
</li>
<li><strong>全连接层 (<code>nn.Linear</code>)</strong>：综合信息做决策。Y&#x3D;X⋅W+b<ul>
<li><em>机械类比：</em> 像一个复杂的<strong>连杆机构</strong>，把输入的所有点映射到输出端。</li>
<li><em>定义：</em> <code>nn.Linear(in_features=100, out_features=10)</code>。输入100个特征，压缩成10个输出（例如10分类）。</li>
</ul>
</li>
<li><strong>Attention层 (<code>nn.MultiheadAttention</code>)</strong>：寻找关联。<ul>
<li><em>逻辑：</em> 类似于在一堆文件中找重点。输入是一句话，它计算词与词之间的相关性。</li>
<li><em>定义：</em> <code>nn.MultiheadAttention(embed_dim=256, num_heads=8)</code>。</li>
<li>分词–查词表，变成向量–<ul>
<li>Attention 的本质是**“加权平均”<strong>，而权重的来源就是</strong>“相关性分数”**。</li>
<li>在数学上，计算两个高维向量相关性最简单、最高效的方法是：<strong>点积 (Dot Product)</strong>。</li>
<li>每个词的向量 x 拆成了三个分身：<ul>
<li><strong>Query (Q):</strong> 我拿着这个去查询别人（比如“我”这个词去查其他词）。</li>
<li><strong>Key (K):</strong> 我作为被查询的对象，展示出来的标签。</li>
<li><strong>Value (V):</strong> 我实际包含的内容信息。</li>
</ul>
</li>
<li>要算“我”和“AI”的相关性：<ul>
<li><strong>Step 1: 计算相似度 (Dot Product)</strong> 拿“我”的 Q 向量，去点乘“AI”的 K 向量</li>
<li><strong>Step 2: 归一化 (Scale &amp; Softmax)</strong> 如果不处理，点积结果可能很大（比如 1000），导致梯度消失。所以除以一个系数，然后过一层 Softmax。 Softmax 会把分数变成<strong>概率分布</strong>（总和为 1)</li>
<li><strong>Step 3: 加权求和 (Weighted Sum)</strong> 最后，用这个概率去乘以每个词的 <strong>Value (V)</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="2-组装动力流-forward"><a href="#2-组装动力流-forward" class="headerlink" title="2. 组装动力流 (forward)"></a>2. 组装动力流 (<code>forward</code>)</h5><p>就是 Tensor 的 <code>.shape</code> 在每一步变成了什么</p>
<p><strong>代码示例：一个简单的图像分类网络</strong></p>
<p><strong>数据流动的形状变化（Shape Analysis）：</strong> 理解 Forward 最关键的是看**Shape（维度）**怎么变。 <code>[32, 1, 28, 28]</code> (输入) -&gt; Conv -&gt; <code>[32, 32, 26, 26]</code> -&gt; Flatten -&gt; <code>[32, 21632]</code> -&gt; Linear -&gt; <code>[32, 10]</code> (输出10个类别的概率)。</p>
<pre><code class="hljs">import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        # --- 1. 定义零件 (定义层) ---
        
    # 卷积层：处理图片，提取特征
    # 输入: [Batch, 1, 28, 28] (比如黑白图片) -&gt; 输出: [Batch, 32, 26, 26]
    self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)
    
    # 全连接层：负责分类
    # 假设经过卷积和池化后，特征数量变成了 32*13*13 = 5408
    self.fc1 = nn.Linear(in_features=5408, out_features=10) 

def forward(self, x):
    # --- 2. 组装动力流 (数据流动路径) ---
    
    # Step 1: 数据 x 进入卷积层
    x = self.conv1(x)  
    
    # Step 2: 经过激活函数 (ReLU)，相当于给电路加个二极管，增加非线性
    x = F.relu(x)      
    
    # Step 3: 展平 (Flatten)
    # 卷积输出是立体的 (C, H, W)，全连接层需要扁平的 (N)
    # 就像把积木拆散铺平
    x = torch.flatten(x, 1) 
    
    # Step 4: 进入全连接层输出结果
    x = self.fc1(x)
    
    return x
</code></pre>
<h4 id="第二部分：改装模型——怎么改开源模型？Adapter是什么？"><a href="#第二部分：改装模型——怎么改开源模型？Adapter是什么？" class="headerlink" title="第二部分：改装模型——怎么改开源模型？Adapter是什么？"></a>第二部分：改装模型——怎么改开源模型？Adapter是什么？</h4><p>在工业界，我们很少从零写上面的 <code>SimpleNet</code>，通常是拿大厂训练好的模型（Pre-trained Model）来改。</p>
<h5 id="场景一：换个“头”-Transfer-Learning"><a href="#场景一：换个“头”-Transfer-Learning" class="headerlink" title="场景一：换个“头” (Transfer Learning)"></a>场景一：换个“头” (Transfer Learning)</h5><p><strong>适用情况：</strong> 用 ResNet 做产品缺陷检测（比如二分类：正常&#x2F;次品），但 ResNet 原生是做 1000 种物体分类的。 <strong>怎么改：</strong> 就是修改中间层、输出层的参数</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import torchvision.models as models<br><br><span class="hljs-comment"># 1. 加载预训练模型 (ResNet18)</span><br>model = models.resnet18(<span class="hljs-attribute">pretrained</span>=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 2. 锁住前面的层 (Frozen)</span><br><span class="hljs-comment"># 就像你买个发动机，不需要重新打磨气缸，只需要调校输出端</span><br><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model.parameters():<br>    param.requires_grad = <span class="hljs-literal">False</span><br><br><span class="hljs-comment"># 3. 修改最后一层 (fc层)</span><br><span class="hljs-comment"># ResNet18 的最后一层叫 .fc，输入是 512</span><br><span class="hljs-comment"># 我们把它换成一个新的 Linear 层，输出变成 2 (只有两类)</span><br>model.fc = nn.Linear(<span class="hljs-attribute">in_features</span>=512, <span class="hljs-attribute">out_features</span>=2)<br><br><span class="hljs-comment"># 此时，训练时只会更新这最后一层的参数，前面不动</span><br></code></pre></td></tr></table></figure>

<h5 id="场景二：插入适配器-Adapter-LoRA"><a href="#场景二：插入适配器-Adapter-LoRA" class="headerlink" title="场景二：插入适配器 (Adapter &#x2F; LoRA)"></a>场景二：插入适配器 (Adapter &#x2F; LoRA)</h5><p><strong>适用情况：</strong> 在不改变原模型（冻结主干）的情况下，插入一些小规模的可训练参数，让模型适应新任务。</p>
<ul>
<li>Sigmoid：汽车油管里的一个单向阀或节流阀。它决定了信号（油）能不能流过去，流多少</li>
<li>Adam：SGD（最基础的优化器）像是每次不管三七二十一都拧一圈。而Adam 像是：“哎，刚才拧猛了，现在慢点拧；这个螺丝比较紧，我要大力一点”。它是动态调整参数更新步长的算法。</li>
<li>Adapter：为了适应大模型新提出来的技术。买了一辆法拉利（大模型），出厂设置锁死了（Frozen），你不舍得也没能力把发动机拆了重装（全量微调）。但是你想让它能跑泥地（适应新任务），于是你在轮轴上加装了一个“越野适配器” (Adapter)。以后你只需要调节这个适配器，不用动法拉利的发动机。</li>
</ul>
<p><strong>目前最火的技术：LoRA (Low-Rank Adaptation)</strong></p>
<p>LoRA 是 Adapter 的一种进化形态。它假设模型微调时的参数变化量是一个“低秩矩阵”。</p>
<ul>
<li><strong>怎么定义 LoRA Adapter？</strong> 它是两个极小的全连接层组成的通路。</li>
<li><strong>怎么插入？</strong> 实际上是与原有的 Linear 层<strong>并联</strong>。</li>
</ul>
<p><strong>代码模拟（手动实现一个 LoRA 层）：</strong></p>
<p>Python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LoRALayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, original_layer, rank=<span class="hljs-number">4</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># 保存原有的层 (冻结住，不让它更新)</span><br>        <span class="hljs-variable language_">self</span>.original_layer = original_layer<br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.original_layer.parameters():<br>            param.requires_grad = <span class="hljs-literal">False</span><br>            <br>        <span class="hljs-comment"># 定义 Adapter (旁路)</span><br>        <span class="hljs-comment"># 假设原层是 1000 -&gt; 1000</span><br>        <span class="hljs-comment"># LoRA A: 1000 -&gt; 4 (降维，压缩信息)</span><br>        <span class="hljs-variable language_">self</span>.lora_a = nn.Linear(original_layer.in_features, rank, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-comment"># LoRA B: 4 -&gt; 1000 (升维，还原信息)</span><br>        <span class="hljs-variable language_">self</span>.lora_b = nn.Linear(rank, original_layer.out_features, bias=<span class="hljs-literal">False</span>)<br>        <br>        <span class="hljs-comment"># 初始化技巧：让旁路一开始输出为0，不影响主模型</span><br>        nn.init.zeros_(<span class="hljs-variable language_">self</span>.lora_b.weight)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># 核心逻辑：结果 = 原路输出 + 旁路输出</span><br>        original_out = <span class="hljs-variable language_">self</span>.original_layer(x)<br>        adapter_out = <span class="hljs-variable language_">self</span>.lora_b(<span class="hljs-variable language_">self</span>.lora_a(x))<br>        <br>        <span class="hljs-keyword">return</span> original_out + adapter_out<br></code></pre></td></tr></table></figure>

<p><strong>工程中的做法（使用 PEFT 库）：</strong> 实际上班时，我们不会手写上面的类，而是使用 HuggingFace 的 <code>peft</code> 库。</p>
<p>Python</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-keyword">from</span> peft import LoraConfig, get_peft_model<br><span class="hljs-keyword">from</span> transformers import AutoModelForCausalLM<br><br><span class="hljs-comment"># 1. 加载大模型</span><br>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-3-8b&quot;</span>)<br><br><span class="hljs-comment"># 2. 定义 Adapter 配置</span><span class="hljs-built_in"></span><br><span class="hljs-built_in">config </span>= LoraConfig(<br>    <span class="hljs-attribute">r</span>=16,             # 秩 (Rank)，决定了Adapter的大小，类似上面代码的 <span class="hljs-attribute">rank</span>=4<br>    <span class="hljs-attribute">lora_alpha</span>=32,    # 缩放系数<br>    target_modules=[<span class="hljs-string">&quot;q_proj&quot;</span>, <span class="hljs-string">&quot;v_proj&quot;</span>], # 指定插入到 Attention 层的哪些部分<br>    <span class="hljs-attribute">task_type</span>=<span class="hljs-string">&quot;CAUSAL_LM&quot;</span><br>)<br><br><span class="hljs-comment"># 3. 一键插入</span><br><span class="hljs-comment"># 这行代码会自动把上面的 LoRALayer 逻辑应用到模型的指定层里</span><br>model = get_peft_model(model, config)<br><br><span class="hljs-comment"># 4. 打印现在的可训练参数量</span><br>model.print_trainable_parameters()<br><span class="hljs-comment"># 输出可能显示：只训练 0.1% 的参数，就能达到微调效果！</span><br></code></pre></td></tr></table></figure>

<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>adapter是一种微调技术，为了让模型能学会某项特定技能</p>
<p>agaent是一种应用架构，为了让模型自主完成复杂任务</p>
<p>rag是信息获取工具，改变的是输入提示词</p>
<h3 id="训练逻辑"><a href="#训练逻辑" class="headerlink" title="训练逻辑"></a>训练逻辑</h3><p>写Python代码来控制它“学习”的过程。</p>
<ul>
<li><strong>具体工作：</strong><ul>
<li><strong>前向传播：</strong> 算出模型的预测值。</li>
<li><strong>计算损失（Loss）：</strong> 用数学公式（如交叉熵）计算预测值和真实值的差距。</li>
<li><strong>反向传播（Backpropagation）：</strong> 这是框架自动做的，但你需要写代码控制优化器（Optimizer）来更新参数。</li>
<li><strong>监控与调试：</strong> 用Python写代码记录训练过程中的准确率、Loss曲线，一旦发现模型不收敛（Loss不下降），需要通过调整学习率或检查代码逻辑来Debug。</li>
</ul>
</li>
</ul>
<p><code>.pt</code> 文件就是用来存 y&#x3D;wx+b 里的 w (权重) 和 b (偏置) 的。但是为了工程便利，我们经常还会存点别的东西：</p>
<ol>
<li><strong>BN 层的统计量 (Running Mean&#x2F;Var)：</strong><ul>
<li>在“批归一化 (Batch Normalization)”层中，不仅有可训练的参数，还有<strong>统计出来的</strong>均值和方差。这些不是通过梯度下降算出来的，而是统计出来的，但也必须存在 <code>.pt</code> 文件里，否则预测时模型会失效。</li>
</ul>
</li>
<li><strong>优化器状态 (Optimizer State)：</strong><ul>
<li>如果你训练了一半停电了，想接着训。你不仅要存模型的权重，还要存<strong>Adam 优化器的状态</strong>（比如当前的动量、步长历史）。</li>
<li><em>这也是为什么有时候 <code>.pt</code> 文件特别大，因为存了两份信息（一份给模型用，一份给优化器接着训用）。</em></li>
</ul>
</li>
</ol>
<h4 id="ONNX"><a href="#ONNX" class="headerlink" title="ONNX"></a>ONNX</h4><p>通用中间文件格式， <code>.pt</code> 文件极其依赖 PyTorch 库。如果你的工业产线电脑只能运行 C++（为了实时性），装不了庞大的 Python 和 PyTorch，你就必须把模型转成 ONNX，然后用 C++ 的 ONNX Runtime 加载运行</p>
<h4 id="tensorRT"><a href="#tensorRT" class="headerlink" title="tensorRT"></a>tensorRT</h4><p>TensorRT是可以在<strong>NVIDIA</strong>各种<strong>GPU硬件平台</strong>下运行的一个<strong>C++推理框架</strong>。我们利用Pytorch、TF或者其他框架训练好的模型，可以转化为TensorRT的格式，然后利用TensorRT推理引擎去运行我们这个模型，从而提升这个模型在英伟达GPU上运行的速度。速度提升的比例是<strong>比较可观</strong>的</p>
<h4 id="工作流-1"><a href="#工作流-1" class="headerlink" title="工作流"></a>工作流</h4><p>在实际的 Jetson 部署中，标准流程是这样的：</p>
<ol>
<li><strong>训练 (PyTorch):</strong> 在服务器上炼丹，得到 <code>.pt</code> 文件（包含 w 和 b）。</li>
<li><strong>转换 (Export):</strong> 用一行代码把 <code>.pt</code> 转成 <code>.onnx</code>。<ul>
<li><em>此时模型变成了通用格式，脱离了 Python 依赖。</em></li>
</ul>
</li>
<li><strong>编译 (Build Engine):</strong> 在你的 <strong>Jetson Orin Nano</strong> 上，使用 TensorRT 把 <code>.onnx</code> 编译成 <code>.engine</code> (或 <code>.trt</code>) 文件。<ul>
<li><em>注意：这一步必须在目标硬件上做！因为它是针对这块具体的显卡定制生成的。</em></li>
</ul>
</li>
<li><strong>推理 (Inference):</strong> 写 C++ 或 Python 代码，加载 <code>.engine</code> 文件，飞快地跑。</li>
</ol>
<p>AI 算法工程师负责让模型更准（产出.pt），AI 部署工程师负责让模型更快（产出 TensorRT Engine）</p>
<p>训练常用代码：     </p>
<pre><code class="hljs">    # 假设 model, train_loader, optimizer, criterion(loss) 都定义好了

	# 1. 开启训练模式 (Switch on)

	model.train() 

	for epoch in range(num_epochs): # 遍历所有数据几轮
    	for batch_idx, (data, target) in enumerate(train_loader):
            # 2. 搬运数据到 GPU (Move to Device)
            data, target = data.to(device), target.to(device)

            # --- 核心三部曲 (面试必问) ---

            # A. 梯度清零 (Reset)
            optimizer.zero_grad()

            # B. 前向传播 + 算账 (Forward + Loss)
            output = model(data)
            loss = criterion(output, target)

            # C. 反向传播 (Backward)
            loss.backward()

            # D. 更新参数 (Step)
            optimizer.step()

            # 3. 监控 (Monitor)
            if batch_idx % 100 == 0:
        		print(f&#39;Epoch {epoch}, Loss: {loss.item()}&#39;)
</code></pre>
<p>这三步操作 <code>zero_grad</code> -&gt; <code>backward</code> -&gt; <code>step</code> 对应着一次完整的<strong>反馈调节</strong>。</p>
<p><strong>考点 A：为什么要 <code>optimizer.zero_grad()</code>？（为什么要清零？）</strong></p>
<ul>
<li><strong>代码含义：</strong> 把上一步算出来的梯度全部扔掉，归零。</li>
<li><strong>物理意义：</strong><ul>
<li>PyTorch 设计的一个机制是**“梯度累加”<strong>。如果你不清零，第二次算的梯度会直接</strong>加**在第一次的梯度上。</li>
</ul>
</li>
</ul>
<p><strong>考点 B：<code>loss.backward()</code> 到底干了什么？</strong></p>
<ul>
<li><strong>代码含义：</strong> 自动求导（Autograd）。</li>
<li><strong>物理意义：</strong><ul>
<li>它构建了一个<strong>计算图 (Computational Graph)</strong>，然后从 Loss 开始，沿着图反向走一遍。</li>
<li>对于模型里的每一个参数 w，它计算出 ∂Loss&#x2F;∂w（偏导数）。</li>
<li>这个偏导数告诉参数：“为了让 Loss 变小，你应该往哪个方向变，变多少？”</li>
<li><strong>结果：</strong> 算完这行代码后，参数本身的值还没变，但是每个参数多了一个 <code>.grad</code> 属性，里面存着它的“整改建议”。</li>
</ul>
</li>
</ul>
<p><strong>考点 C：<code>optimizer.step()</code> 到底干了什么？</strong></p>
<ul>
<li><strong>代码含义：</strong> 执行更新。</li>
<li><strong>物理意义：</strong><ul>
<li>这时候 <strong>Adam</strong> 或 <strong>SGD</strong> 登场了。它读取每个参数的 <code>.grad</code>（整改建议），然后根据学习率（Learning Rate）真正去修改参数的值。</li>
</ul>
</li>
</ul>
<h4 id="A-检查点管理-Checkpoints-Model-Saving"><a href="#A-检查点管理-Checkpoints-Model-Saving" class="headerlink" title="A. 检查点管理 (Checkpoints &#x2F; Model Saving)"></a><strong>A. 检查点管理 (Checkpoints &#x2F; Model Saving)</strong></h4><p><strong>问题：</strong> 训练了3天，突然停电了&#x2F;显存爆了，怎么办？白跑了？ <strong>技能：</strong> 你需要写代码，每隔几个 Epoch 就把模型保存下来。</p>
<p>Python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 保存的不仅仅是模型参数，还有优化器的状态（比如动量信息）</span><br>torch.save(&#123;<br>    <span class="hljs-string">&#x27;epoch&#x27;</span>: epoch,<br>    <span class="hljs-string">&#x27;model_state_dict&#x27;</span>: model.state_dict(),<br>    <span class="hljs-string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),<br>    <span class="hljs-string">&#x27;loss&#x27;</span>: loss,<br>&#125;, <span class="hljs-string">f&quot;checkpoint_epoch_<span class="hljs-subst">&#123;epoch&#125;</span>.pt&quot;</span>)<br></code></pre></td></tr></table></figure>

<ul>
<li><em>面试加分点：</em> 知道保存 <code>optimizer</code> 的状态很重要，否则恢复训练时，Adam 会失去之前的“手感”（动量历史），导致 Loss 突变。</li>
</ul>
<h4 id="B-可视化监控-TensorBoard-WandB"><a href="#B-可视化监控-TensorBoard-WandB" class="headerlink" title="B. 可视化监控 (TensorBoard &#x2F; WandB)"></a><strong>B. 可视化监控 (TensorBoard &#x2F; WandB)</strong></h4><p><strong>问题：</strong> 面对满屏滚动的 Loss 数字（2.345, 2.341, 2.339…），你怎么知道模型是不是收敛了？有没有过拟合？ <strong>技能：</strong> 使用 TensorBoard 或 Weights &amp; Biases (WandB) 画图。</p>
<ul>
<li><strong>看 Loss 曲线：</strong> 是平滑下降？还是剧烈震荡（学习率太大了）？还是完全不动（梯度消失了）？</li>
<li><strong>看梯度分布：</strong> 梯度是不是变成 0 了？</li>
</ul>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>在求职中，当面试官问你“是否熟悉Python模型开发”时，他考察的其实是：</p>
<ol>
<li><strong>框架熟练度：</strong> 你能不能熟练使用 PyTorch &#x2F; TensorFlow 的API？</li>
<li><strong>调试能力：</strong> 当模型报错“Dimension mismatch（维度不匹配）”或“OOM（显存溢出）”时，你知道去哪里改代码吗？</li>
<li><strong>复现能力：</strong> 给你一篇新论文，你能看着公式把它变成上面的Python代码吗？</li>
</ol>
<h1 id="C-用于部署加速和底层控制"><a href="#C-用于部署加速和底层控制" class="headerlink" title="C++用于部署加速和底层控制"></a>C++用于部署加速和底层控制</h1>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/" class="print-no-link">#学习路线</a>
      
        <a href="/tags/AI/" class="print-no-link">#AI</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>AI学习路线--学习路线20260127</div>
      <div>http://example.com/2026/01/27/AI学习路线-学习路线20260127/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>January 27, 2026</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2026/01/27/AI%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF-%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE-1/" title="AI学习路线--部署项目(1)">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">AI学习路线--部署项目(1)</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/12/25/%E8%AE%BA%E6%96%87%E6%9F%A5%E6%89%BE%E4%B8%8E%E4%B8%8B%E8%BD%BD/" title="论文查找与下载">
                        <span class="hidden-mobile">论文查找与下载</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
